{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUylUoJHdRg1"
   },
   "source": [
    "### Dataset and envs\n",
    "1. Mount google drive.\n",
    "2. Declare global variables.\n",
    "3. Export source code folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "Uin8FKF-dN-x",
    "outputId": "bd791e67-ff88-461f-e7b9-2cf1d482d8c0"
   },
   "outputs": [],
   "source": [
    "GEN_DATA_DIR = '../../vae-dec/data/gene'\n",
    "LOG_DIR = '../output/log'\n",
    "MODEL_DIR = '../data/results/model'\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# MODEL_DIR = '/content/drive/My Drive/DL/models'\n",
    "# LOG_DIR = '/content/drive/My Drive/DL/log/vae-dec'\n",
    "# GEN_DATA_DIR = '/content/drive/My Drive/DL/data/gene/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkXY_GVQdWTo"
   },
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "ih1fO2s_daHY",
    "outputId": "43937423-3711-49c5-90fc-604a08dfba4d"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
    "\n",
    "# For metagenomics dataset\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# import nxmetis\n",
    "import copy\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import re, os\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../vae-dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_reads(filename, type='fasta'):\n",
    "    def format_read(read):\n",
    "        # Return sequence and label\n",
    "        z = re.split('[|={,]+', read.description)\n",
    "        return read.seq, z[3]\n",
    "    try:\n",
    "        seqs = list(SeqIO.parse(filename, type))\n",
    "        reads = []\n",
    "        labels = []\n",
    "\n",
    "        # Detect for paired-end or single-end reads\n",
    "        # If the id of two first reads are different (e.g.: .1 and .2), they are paired-end reads\n",
    "        is_paired_end = False\n",
    "        if len(seqs) > 2 and seqs[0].id[-1:] != seqs[1].id[-1:]:\n",
    "            is_paired_end = True\n",
    "\n",
    "        label_list = dict()\n",
    "        label_index = 0\n",
    "\n",
    "        for i in range(0, len(seqs), 2 if is_paired_end else 1):\n",
    "            read, label = format_read(seqs[i])\n",
    "            if is_paired_end:\n",
    "                read2, label2 = format_read(seqs[i + 1])\n",
    "                read += read2\n",
    "            reads += [str(read)]\n",
    "\n",
    "            # Create labels\n",
    "            if label not in label_list:\n",
    "                label_list[label] = label_index\n",
    "                label_index += 1\n",
    "            labels.append(label_list[label])\n",
    "\n",
    "        del seqs\n",
    "\n",
    "        return reads, labels\n",
    "    except Exception as e:\n",
    "        print('Error when loading file {} '.format(filename))\n",
    "        print('Cause: ', e)\n",
    "        return []\n",
    "\n",
    "def gen_kmers(klist):\n",
    "    '''\n",
    "    Generate list of k-mer words. Given multiple k-mer values.\n",
    "    Args:\n",
    "        klist: list of k-mer value\n",
    "    Return:\n",
    "        List of k-mer words\n",
    "    '''\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    kmers_list = []\n",
    "    for k in klist:\n",
    "        kmers_list += [''.join(p) for p in it.product(bases, repeat=k)]\n",
    "\n",
    "    # reduce a half of k-mers due to symmetry\n",
    "    kmers_dict = dict()\n",
    "    for myk in kmers_list:\n",
    "        k_reverse_complement=Seq(myk).reverse_complement()\n",
    "        if not myk in kmers_dict and not str(k_reverse_complement) in kmers_dict:\n",
    "            kmers_dict[myk]=0\n",
    "\n",
    "    return list(kmers_dict.keys())\n",
    "\n",
    "def create_document( reads, klist):\n",
    "    \"\"\"\n",
    "    Create a set of document from reads, consist of all k-mer in each read\n",
    "    For example:\n",
    "    k = [3, 4, 5]\n",
    "    documents =\n",
    "    [\n",
    "        'AAA AAT ... AAAT AAAC ... AAAAT AAAAC' - read 1\n",
    "        'AAA AAT ... AAAT AAAC ... AAAAT AAAAC' - read 2\n",
    "        ...\n",
    "        'AAA AAT ... AAAT AAAC ... AAAAT AAAAC' - read n\n",
    "    ]\n",
    "    :param reads:\n",
    "    :param klist: list of int\n",
    "    :return: list of str\n",
    "    \"\"\"\n",
    "    # create a set of document\n",
    "    documents = []\n",
    "    for read in reads:\n",
    "        k_mers_read = []\n",
    "        for k in klist:\n",
    "            k_mers_read += [read[j:j + k] for j in range(0, len(read) - k + 1)]\n",
    "        documents.append(k_mers_read)\n",
    "\n",
    "    k_mers_set = [gen_kmers(klist)]\n",
    "    dictionary = corpora.Dictionary(k_mers_set)\n",
    "    return dictionary, documents\n",
    "\n",
    "def save_documents(documents, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for d in documents:\n",
    "            f.write(\"%s\\n\" % d)\n",
    "\n",
    "\n",
    "def parallel_create_document(reads, klist, n_workers=2 ):\n",
    "    \"\"\"\n",
    "    Create a set of document from reads, consist of all k-mer in each read\n",
    "    For example:\n",
    "    k = [3, 4, 5]\n",
    "    documents =\n",
    "    [\n",
    "        'AAA AAT ... AAAT AAAC ... AAAAT AAAAC' - read 1\n",
    "        'AAA AAT ... AAAT AAAC ... AAAAT AAAAC' - read 2\n",
    "        ...\n",
    "        'AAA AAT ... AAAT AAAC ... AAAAT AAAAC' - read n\n",
    "    ]\n",
    "    :param reads:\n",
    "    :param klist: list of int\n",
    "    :return: list of str\n",
    "    \"\"\"\n",
    "\n",
    "    # create k-mer dictionary\n",
    "    k_mers_set = [gen_kmers( klist )] #[genkmers(val) for val in klist]\n",
    "    dictionary = corpora.Dictionary(k_mers_set)\n",
    "\n",
    "    documents = []\n",
    "    reads_str_chunk = [list(item) for item in np.array_split(reads, n_workers)]\n",
    "    chunks = [(reads_str_chunk[i], klist) for i in range(n_workers)]\n",
    "    pool = Pool(processes=n_workers)\n",
    "\n",
    "    result = pool.starmap(create_document, chunks)\n",
    "    for item in result:\n",
    "        documents += item\n",
    "    return dictionary, documents\n",
    "\n",
    "def create_corpus(dictionary: corpora.Dictionary, documents, \n",
    "                  is_tfidf=False, \n",
    "                  smartirs=None, \n",
    "                  is_log_entropy=False, \n",
    "                  is_normalize=True):\n",
    "    corpus = [dictionary.doc2bow(d, allow_update=False) for d in documents]\n",
    "    if is_tfidf:\n",
    "        tfidf = TfidfModel(corpus=corpus, smartirs=smartirs)\n",
    "        corpus = tfidf[corpus]\n",
    "    elif is_log_entropy:\n",
    "        log_entropy_model = LogEntropyModel(corpus, normalize=is_normalize)\n",
    "        corpus = log_entropy_model[corpus]\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def compute_kmer_dist(dictionary, corpus, groups, seeds, only_seed=True):\n",
    "    corpus_m = gensim.matutils.corpus2dense(corpus, len(dictionary.keys())).T\n",
    "    res = []\n",
    "    if only_seed:\n",
    "        for seednodes in seeds:\n",
    "            tmp = corpus_m[seednodes, :]\n",
    "            res += [np.mean(tmp, axis=0)]\n",
    "    else:\n",
    "        for groupnodes in groups:\n",
    "            tmp = corpus_m[groupnodes, :]\n",
    "            res += [np.mean(tmp, axis=0)]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sejBAYWJjfU2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def genome_acc(grps, pred_grps, y_true, n_cluters):\n",
    "    groups_cluster_lb = assign_cluster_2_reads(grps, pred_grps)\n",
    "    prec, recall = eval_quality(y_true, groups_cluster_lb, n_clusters=n_cluters)\n",
    "    f1_score = 2*((prec*recall)/(prec+recall))\n",
    "    return prec, recall, f1_score\n",
    "    \n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9tlp3Dgd7xY"
   },
   "source": [
    "#### Function for creating stable set\n",
    "1. build_overlap_graph: build a graph of overlapping k-mers.\n",
    "2. make_stable_set: a greey method for creating groups (connected components) and seeds (independent set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cS8qTxKsfal8"
   },
   "outputs": [],
   "source": [
    "def build_overlap_graph(reads, labels, qmer_length, num_shared_reads):\n",
    "    '''\n",
    "    Build overlapping graph\n",
    "    '''\n",
    "    # Create hash table with q-mers are keys\n",
    "    print(\"Building hash table...\")\n",
    "    lmers_dict=dict()\n",
    "    for idx, r in enumerate(reads):\n",
    "        for j in range(0,len(r)-qmer_length+1):\n",
    "            lmer = r[j:j+qmer_length]\n",
    "            if lmer in lmers_dict:\n",
    "                lmers_dict[lmer] += [idx]\n",
    "            else:\n",
    "                lmers_dict[lmer] = [idx]\n",
    "\n",
    "    print('Building edges...')\n",
    "    E=dict()\n",
    "    for lmer in lmers_dict:\n",
    "        for e in it.combinations(lmers_dict[lmer],2):\n",
    "            if e[0]!=e[1]:\n",
    "                e_curr=(e[0],e[1])\n",
    "            else:\n",
    "                continue\n",
    "            if e_curr in E:\n",
    "                E[e_curr] += 1 # Number of connected lines between read a and b\n",
    "            else:\n",
    "                E[e_curr] = 1\n",
    "    E_Filtered = {kv[0]: kv[1] for kv in E.items() if kv[1] >= num_shared_reads}\n",
    "    \n",
    "    print('Building graph...')\n",
    "    G = nx.Graph()\n",
    "    print('Adding nodes...')\n",
    "    color_map = {0: 'red', 1: 'green', 2: 'blue', 3: 'yellow', 4: 'darkcyan', 5: 'violet'}\n",
    "    for i in range(0, len(labels)):\n",
    "        G.add_node(i, label=labels[i], color=color_map[labels[i]])\n",
    "\n",
    "    print('Adding edges...')\n",
    "    for kv in E_Filtered.items():\n",
    "        G.add_edge(kv[0][0], kv[0][1], weight=kv[1])\n",
    "    print('Graph constructed!')\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40jBPsnXdp4z"
   },
   "outputs": [],
   "source": [
    "def is_seed(neighbors, seed):\n",
    "    for n_node in neighbors:\n",
    "        if n_node in seed:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def make_stable_set(G: nx.Graph, maximum_seeds):\n",
    "    GL = []\n",
    "    SL = []\n",
    "    temp_G = copy.copy(G)\n",
    "    traversed_nodes = []\n",
    "    for i, node in enumerate(temp_G.nodes):\n",
    "        if node in traversed_nodes:\n",
    "            continue\n",
    "        SGi = []\n",
    "        Gi = []\n",
    "        SGi.append(node)\n",
    "        Gi.append(node)\n",
    "        traversed_nodes.append(node)\n",
    "        for grp_node in Gi:\n",
    "            neighbors = list(temp_G.neighbors(grp_node))\n",
    "            if neighbors:\n",
    "                add_node = None\n",
    "                for n_node in neighbors:\n",
    "                    if n_node not in traversed_nodes:\n",
    "                        add_node = n_node\n",
    "                        traversed_nodes.append(n_node)\n",
    "                        break\n",
    "\n",
    "                # random_first_neighbor = neighbors[0]\n",
    "                # # temp_G.remove_node(random_first_neighbor)\n",
    "                # traversed_nodes.append(random_first_neighbor)\n",
    "                if add_node:\n",
    "                    neighbors.remove(add_node)\n",
    "                    if is_seed(neighbors, SGi):\n",
    "                        SGi.append(add_node)\n",
    "                    Gi.append(add_node)\n",
    "\n",
    "            if (len(SGi) >= maximum_seeds) or (not temp_G.nodes):\n",
    "                break\n",
    "\n",
    "        SL.append(SGi)\n",
    "        GL.append(Gi)\n",
    "\n",
    "    return GL, SL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bxzke3g1e3Ve"
   },
   "source": [
    "#### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBHAdBg7e5ZN"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder, normalize, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from dataloader.utils import load_meta_reads, compute_kmer_dist, create_document, create_corpus\n",
    "class GenomeDataset_v3():\n",
    "    '''\n",
    "    Metagenomics dataset for reading simulated data in fasta format (.fna)\n",
    "    An optimization step based on graph opertation is used to merge reads that\n",
    "    have overlapping reads into a completed genome.\n",
    "    '''\n",
    "    def __init__(self, fna_file, kmers: list, qmers, num_shared_reads, maximum_group_size=5000,\n",
    "                     maximum_seed_size=200, only_seed=False, is_normalize=True,\n",
    "                     graph_file=None, is_serialize=False, is_deserialize=False):\n",
    "        '''\n",
    "        Args:\n",
    "            kmers: a list of kmer values. \n",
    "            fna_file: path to fna file (fasta format).\n",
    "            only_seed: only seeds in overlapping graph are used to build features.\n",
    "            graph_file: calculated groups and seeds (json).\n",
    "        '''\n",
    "        # Read fasta dataset\n",
    "        print('Reading fna file...')\n",
    "        self.reads, self.labels = load_meta_reads(fna_file, type='fasta')\n",
    "        # self.reads = self.reads[:5000]\n",
    "        # self.labels = self.labels[:5000]\n",
    "\n",
    "        print('Creating document from reads...')\n",
    "        dictionary, documents = create_document(self.reads, kmers)\n",
    "\n",
    "        print('Creating corpus...')\n",
    "        corpus = create_corpus(dictionary, documents)\n",
    "\n",
    "        self.groups = []\n",
    "        self.seeds = []\n",
    "\n",
    "        if is_deserialize:\n",
    "            print('Deserializing data...')\n",
    "            self.groups, self.seeds = self.deserialize_data(graph_file, self.reads)\n",
    "        else:\n",
    "            # Build overlapping (reads) graph\n",
    "            print('Building graph from scratch...')\n",
    "            graph = build_overlap_graph(self.reads, self.labels, qmers, num_shared_reads=num_shared_reads)\n",
    "            print('Partitioning graph...')\n",
    "            self.groups, self.seeds = make_stable_set(graph, maximum_seed_size)\n",
    "\n",
    "        if is_serialize:\n",
    "            print('Serializing data to...', graph_file)\n",
    "            self.serialize_data(self.reads, self.groups, self.seeds, graph_file)\n",
    "\n",
    "        print('Computing features...')\n",
    "        self.kmer_features = compute_kmer_dist(dictionary, corpus, self.groups, self.seeds, only_seed=only_seed)\n",
    "        self.groups_kmer_features = compute_kmer_dist(dictionary, corpus, self.groups, self.seeds, only_seed=False)\n",
    "\n",
    "        if is_normalize:\n",
    "            print('Normalizing...')\n",
    "            scaler = StandardScaler()\n",
    "            self.kmer_features = scaler.fit_transform(self.kmer_features)\n",
    "\n",
    "        print('Finish.')\n",
    "    \n",
    "    def serialize_data(self, reads, groups, seeds, graph_file):\n",
    "        serialize_dict = {\n",
    "            'groups': groups,\n",
    "            'seeds': seeds\n",
    "        }\n",
    "\n",
    "        with open(graph_file, 'w') as fg:\n",
    "            json.dump(serialize_dict, fg)\n",
    "        \n",
    "        return graph_file\n",
    "    \n",
    "    def deserialize_data(self, graph_file, reads):\n",
    "        with open(graph_file, 'r') as fg:\n",
    "            data = json.load(fg)\n",
    "\n",
    "        groups = data['groups']\n",
    "        seeds = data['seeds']\n",
    "\n",
    "        return groups, seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uoSw_z0Sfm3P"
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8V5wEolNfvsO"
   },
   "source": [
    "##### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2tAR9Pvfn5H"
   },
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iVe2OaYIfzAm"
   },
   "source": [
    "##### Clustering layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU5-AFStf1A8"
   },
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1fOy-qVf5mz"
   },
   "source": [
    "##### Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sb16FPCIf72m"
   },
   "outputs": [],
   "source": [
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, grps=None, n_clusters=2, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if int(epochs/10) != 0 and epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    # print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                    #       % (metrics.acc(self.y, y_pred), metrics.nmi(self.y, y_pred)))\n",
    "\n",
    "                    print(' '*8 + '|==>  acc: %.4f <==|' % (genome_acc(grps, y_pred, self.y, n_clusters)[1]))\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time.time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: %ds' % round(time.time() - t0))\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, grps=None, n_clusters=2, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        best_model = self.model\n",
    "        best_acc = 0.0\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = int(x.shape[0] / batch_size) * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time.time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'precision', 'recall', 'f1_score', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    # acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "                    # nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "                    # ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "                    prec, recall, f1_score = genome_acc(grps, y_pred, y, n_clusters)\n",
    "                    if f1_score > best_acc:\n",
    "                      best_acc = f1_score\n",
    "                      best_model = self.model\n",
    "                    loss = np.round(loss, 5)\n",
    "                    # logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logdict = dict(iter=ite, precision=prec, recall=recall, f1_score=f1_score, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    # print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "                    print('Iter %d: precision = %.5f, recall = %.5f, f1_score = %.5f,\\\n",
    "                            nmi = --, ari = --' % (ite, prec, recall, f1_score), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            loss = self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        best_model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tl3snWJ3gNOv"
   },
   "source": [
    "#### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OU24olXJgPKi"
   },
   "outputs": [],
   "source": [
    "def load_genomics(dataset_name,\n",
    "                    kmers, \n",
    "                    lmer,\n",
    "                    maximum_seed_size,\n",
    "                    num_shared_reads,\n",
    "                    graph_file=None,\n",
    "                    is_serialize=False,\n",
    "                    is_deserialize=False,\n",
    "                    is_normalize=False,\n",
    "                    only_seed=False):\n",
    "    '''\n",
    "    Loads fna file.\n",
    "    Args:\n",
    "        dataset_name: name of dataset (e.g. S1.fna, L1.fna,...)\n",
    "        kmers: list of kmers.\n",
    "        lmer: lmer.\n",
    "        maximum_seed_size.\n",
    "        num_shared_reads.\n",
    "        graph_file: computed groups/seeds json file.\n",
    "        is_serialize: True to serialize computed groups/seeds to json file.\n",
    "        is_deserialize: True to load computed groups/seeds in json file.\n",
    "        is_normalize: whether to normalize kmer-features.\n",
    "        only_seed: True to compute kmer features using seeds only.\n",
    "    '''\n",
    "    genomics_dataset = GenomeDataset_v3(\n",
    "        dataset_name, kmers, lmer,\n",
    "        graph_file=graph_file,\n",
    "        only_seed=only_seed,\n",
    "        maximum_seed_size=maximum_seed_size,\n",
    "        num_shared_reads=num_shared_reads,\n",
    "        is_serialize=is_serialize,\n",
    "        is_deserialize=is_deserialize,\n",
    "        is_normalize=is_normalize)\n",
    "\n",
    "    return genomics_dataset.kmer_features,\\\n",
    "        genomics_dataset.groups_kmer_features,\\\n",
    "        genomics_dataset.labels,\\\n",
    "        genomics_dataset.groups,\\\n",
    "        genomics_dataset.seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ApdCpsk1rzom"
   },
   "source": [
    "#### Inspects stable set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DedRNq_Frzoz"
   },
   "source": [
    "##### TSNE visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDmbY784rzo0"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wu1SzYtYrzo2"
   },
   "outputs": [],
   "source": [
    "def visualize(x, labels, n_clusters, range_lim=(-20, 20), perplexity=40):\n",
    "    tsne = TSNE(n_components=n_clusters, verbose=1, perplexity=perplexity, n_iter=400, init='pca')\n",
    "    tsne_results = tsne.fit_transform(x)\n",
    "    df_subset = pd.DataFrame()\n",
    "    df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "    df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "    df_subset['Y'] = labels\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sns.scatterplot(\n",
    "        x='tsne-2d-one', y='tsne-2d-two',\n",
    "        hue='Y',\n",
    "        palette=sns.color_palette(n_colors=n_clusters),\n",
    "        data=df_subset,\n",
    "        legend=\"full\",\n",
    "        alpha=0.3\n",
    "    ).set(xlim=range_lim,ylim=range_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXF_4eVBgYgT"
   },
   "source": [
    "#### Training parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFPPJ6ergbMe"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "MAX_ITERS = 1000\n",
    "PRETRAIN_EPOCHS = 20\n",
    "UPDATE_INTERVAL = 30\n",
    "TOL = 0.0001\n",
    "\n",
    "AE_WEIGHTS = None\n",
    "AE_WEIGHTS = '../data/results/model\\dec-keras\\gen\\S1/ae_weights.h5'\n",
    "DATASET_DIR = GEN_DATA_DIR\n",
    "DATASET_NAME = 'S1'\n",
    "SAVE_DIR = os.path.join(MODEL_DIR, 'dec-keras', 'gen', DATASET_NAME)\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "# Hyperparameters\n",
    "KMERS = [4]\n",
    "N_CLUSTERS = 2\n",
    "LMER = 20\n",
    "NUM_SHARED_READS = 5\n",
    "ONLY_SEED = True\n",
    "MAXIMUM_SEED_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "Y2lGiSIChiro",
    "outputId": "ada7ef8e-685e-462a-a4c9-521392b4a785",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_kmer_features, group_kmer_features, labels, groups, seeds = load_genomics(\n",
    "        os.path.join(DATASET_DIR, DATASET_NAME + '.fna'),\n",
    "        kmers=KMERS,\n",
    "        lmer=LMER,\n",
    "        maximum_seed_size=MAXIMUM_SEED_SIZE,\n",
    "        num_shared_reads=NUM_SHARED_READS,\n",
    "        is_deserialize=True,\n",
    "        is_serialize=True,\n",
    "        is_normalize=True,\n",
    "        only_seed=ONLY_SEED,\n",
    "        graph_file=os.path.join(DATASET_DIR, 'processed', DATASET_NAME + '.json')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "qeR4_Q7ThV_x",
    "outputId": "215e8422-9ec0-4ad6-c74f-b5e1a076be4a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                               distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
    "pretrain_optimizer = SGD(lr=1.0, momentum=0.9)\n",
    "dec = DEC(dims=[seed_kmer_features.shape[-1], 500, 500, 2000, 10], n_clusters=N_CLUSTERS, init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "id": "Vpqrrxuviq0H",
    "outputId": "3b83f2fd-1968-46df-ca31-dda4ca0ff287",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if AE_WEIGHTS is None:\n",
    "    dec.pretrain(x=seed_kmer_features, y=labels, grps=groups, optimizer=pretrain_optimizer,\n",
    "                epochs=PRETRAIN_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                save_dir=SAVE_DIR)\n",
    "else:\n",
    "    dec.autoencoder.load_weights(AE_WEIGHTS)\n",
    "\n",
    "dec.model.summary()\n",
    "t0 = time.time()\n",
    "dec.compile(optimizer=SGD(0.1, 0.9), loss='kld')\n",
    "y_pred = dec.fit(x=seed_kmer_features, y=labels, grps=groups, n_clusters=N_CLUSTERS, tol=TOL, maxiter=MAX_ITERS, batch_size=BATCH_SIZE,\n",
    "                  update_interval=UPDATE_INTERVAL, save_dir=SAVE_DIR)\n",
    "print('acc:', genome_acc(groups, y_pred, labels, N_CLUSTERS)[1])\n",
    "print('clustering time: ', (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Tu8ECMzrzpF"
   },
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3F92mu_vrzpG"
   },
   "outputs": [],
   "source": [
    "# Assign groups labels from reads labels\n",
    "# Dominant number of each label is the label of group\n",
    "# [0 0 0 0 1 1 0] -> group has label 0 (5 > 2)\n",
    "grps_label = []\n",
    "for group in groups:\n",
    "    lb_type = {}\n",
    "    for node in group:\n",
    "        if labels[node] not in list(lb_type.keys()):\n",
    "            lb_type[labels[node]] = 1\n",
    "        else:\n",
    "            lb_type[labels[node]] += 1\n",
    "    max_val = 0\n",
    "    key = -1\n",
    "    for kv in lb_type.items():\n",
    "        if kv[1] > max_val:\n",
    "            max_val = kv[1]\n",
    "            key = kv[0]\n",
    "    if key != -1:\n",
    "        grps_label.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9TH3ZumrzpJ"
   },
   "outputs": [],
   "source": [
    "visualize(seed_kmer_features, grps_label, N_CLUSTERS, (-20, 20), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CC36PsrrzpL"
   },
   "outputs": [],
   "source": [
    "visualize(group_kmer_features, grps_label, N_CLUSTERS, (-20, 20), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gtruth of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "zvI24h9XrzpO",
    "outputId": "c7594b77-0fc9-4fcf-c416-16de37e1bff7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent = dec.encoder.predict(seed_kmer_features)\n",
    "visualize(latent, grps_label, N_CLUSTERS, (-50, 50), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxsqPTF7rzpR"
   },
   "outputs": [],
   "source": [
    "latent = dec.encoder.predict(seed_kmer_features)\n",
    "y_pred = dec.predict(seed_kmer_features)\n",
    "visualize(latent, y_pred, N_CLUSTERS, (-50, 50), 20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "keras_dec-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
